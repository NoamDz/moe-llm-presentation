# Mixture-of-Experts LLM PresentationAdd commentMore actions

This repository accompanies a presentation on the usage of Mixture of Experts (MoE) techniques for large language models. It includes slides, illustrative figures and example code for experimenting with MoE layers.

## Contents

- **presentation.ipynb** – Jupyter notebook that outlines the concepts behind MoE, covering Switch Transformer.
- **plots** – figures used in the presentation.

## License

This material is provided for educational purposes and does not include pretrained models.Add comment
