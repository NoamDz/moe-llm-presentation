{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28700274-faf1-4b4a-ba3c-159a77d81563",
   "metadata": {},
   "source": [
    "# Mixture of experts Language Model presentation\n",
    "\n",
    "## Noam Delbari & Stav Cohen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c93a4-e500-4480-a9fe-16e50791a9f6",
   "metadata": {},
   "source": [
    "## 1 Background:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda35158-1a66-4354-9346-92b5bbd3ef43",
   "metadata": {},
   "source": [
    "### 1.1 Standard Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe019de3-0300-4285-9dcd-6182400f5e56",
   "metadata": {},
   "source": [
    "- Encoder/decoder stacks, multi-head self-attention, position encodings, residual + layer-norm.\n",
    "- Emphasize the Feed-Forward Network (FFN) accounting for ~⅔ of parameters and FLOPs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16ecc2-ad9d-4f1c-9bfe-788abe1f2d04",
   "metadata": {},
   "source": [
    "### 1.2 Mixture-of-Experts (MoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d106ada-a163-49f4-82af-ba3b2dd9bb2a",
   "metadata": {},
   "source": [
    "- Multiple specialist sub-nets compete; a gating network softly/hard-routes input to the best expert(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e70a8c-89fa-4649-83ba-0e205befe346",
   "metadata": {},
   "source": [
    "## 2 Switch transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6283fa5b-7516-441a-95f4-8eb1c6c07fe3",
   "metadata": {},
   "source": [
    "### 2.1 Introduction: Motivation & Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da460eef-f899-48bd-a268-f0a3cd93fb45",
   "metadata": {},
   "source": [
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee1062-c3b6-4348-8786-97806bcf7eb2",
   "metadata": {},
   "source": [
    "### 2.2 Description & Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7863ffea-8996-4830-aa5e-20366d479d78",
   "metadata": {},
   "source": [
    "- Switch Transformer extends the standard Transformer by replacing each dense feed-forward network with a Mixture-of-Experts layer which is being enabled by a lightweight gating network, letting only 1–2 experts process every token. This design keeps inference cost comparable to the dense baseline while unlocking trillion-parameter scale and superior quality on language, translation and multitask benchmarks\n",
    "- Hands-on code snippets of the mode implementation, using equations and explanations: Forward pass, Switch Transformer Block and Switch Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd2cfbd-cef2-4fff-adac-cf97bd4b99dd",
   "metadata": {},
   "source": [
    "### 2.3 Experiments & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9892016-e26e-44c5-8fa9-e5684e2ec41b",
   "metadata": {},
   "source": [
    "- Presenting Scaling & Efficiency results to motivate why use sparse experts\n",
    "- Presenting  Ablation & Stability results to show that gating mechanism is capable for training large scale models\n",
    "- Downstream fine-tuning results to show that the lower perplexity (compared to dense) during pre training has an effect over downstream tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6985c725-08ca-4c7b-9b5e-6c65869db759",
   "metadata": {},
   "source": [
    "### 2.4 Hands-on experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec98893e-7dd9-419b-9438-3f4903c40d75",
   "metadata": {},
   "source": [
    "- Forward pass: Configure single expert per token and plot router decisions (Specificity of token types per expert)\n",
    "- Training: Comparing switch model to dense baseline with equal number of active parameters and show that sparse model reaches lower validation loss faster\n",
    "- Scaling and Efficiency miniature run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57167407-c9a5-429f-87fb-1de5c50a6951",
   "metadata": {},
   "source": [
    "## 3 PEER (parameter efficient expert retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae322a-6a7f-4b44-b5e4-57bc15d704a5",
   "metadata": {},
   "source": [
    "### 3.1 Introduction and motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5675abf-bf46-4439-9efb-4f57ee397fd3",
   "metadata": {},
   "source": [
    "-  Following paper https://arxiv.org/pdf/2407.04153v1 gives an innovation of sparse mixture-of-experts (MoE) model architectures using PEER (parameter efficient expert retrieval) layer.\n",
    "The PEER layer includes a vast number of tiny expert (over a million) and solves the issue of the linear increase in computational costs and activation memory as the hidden layer width grows in the traditional models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de9cd4-23cb-4084-9c74-a7fbe21d5623",
   "metadata": {},
   "source": [
    "### 3.2 Architecture and mathematical equalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132de35-4f8d-40b5-b873-7f9a41bffd7d",
   "metadata": {},
   "source": [
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5012373d-bae6-4434-a232-692cf8c59c60",
   "metadata": {},
   "source": [
    "### 3.3 Results of major experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab50e99-1c66-4491-84d1-2a400f6cd1ac",
   "metadata": {},
   "source": [
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b5ff6-5f7c-42fa-ace0-fc9eea195c76",
   "metadata": {},
   "source": [
    "### 3.4 Hands-on examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01708c1d-443c-40b1-8c28-112c66e93112",
   "metadata": {},
   "source": [
    "-  Some hands on examples of basic peer layer implementation with the execution output in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a63e7d-904a-496b-a20f-0a7d0171ece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here we're going to make hands-on examples\n"
     ]
    }
   ],
   "source": [
    "print(\"Here we're going to make hands-on examples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
