{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135ae95f",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/stavco9/moe-llm-presentation/blob/main/presentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28700274-faf1-4b4a-ba3c-159a77d81563",
   "metadata": {
    "id": "28700274-faf1-4b4a-ba3c-159a77d81563"
   },
   "source": [
    "# Seminar in Large Language Models and Information Theory (3968)\n",
    "\n",
    "**Master’s Program · Computer Science**  \n",
    "_Reichman University_\n",
    "\n",
    "---\n",
    "\n",
    "**Presenters** · Noam Delbari & Stav Cohen  \n",
    "**Supervisor** · Dr. Alon Kipnis\n",
    "\n",
    "# Mixture-of-Experts Language Models – High-Level Overview\n",
    "\n",
    "Modern large language models (LLMs) can exceed **100 B parameters**, yet at inference time they only use a small slice of that capacity. **Mixture-of-Experts (MoE)** layers make this efficiency explicit: instead of applying one huge feed-forward network to every token, we keep a *team* of specialised experts and let a lightweight **router** choose the two or three best ones on-the-fly. The result is a model that *behaves* like a colossal dense transformer but *runs* like a much smaller network.\n",
    "\n",
    "This presentation will guide you from the basics to cutting-edge MoE tricks:\n",
    "\n",
    "1. **Transformers in 5 minutes** – the encoder–decoder “conveyor belt” and why self-attention scales as \\(O(N^2)\\).  \n",
    "2. **Why Mixture-of-Experts?** – intuition, historical roots, and how sparsity slashes compute while boosting capacity.  \n",
    "3. **Switch Transformer** – the first practical sparse MoE layer and its load-balancing loss.  \n",
    "4. **State of the art** – Mixtral, DeepSeek, and PEER: top-2 gating, sub-experts, and retrieval-aware routing.  \n",
    "5. **Hands-on demo** – training a toy MoE in PyTorch and visualising how tokens find their experts.  \n",
    "6. **Take-aways** – when to reach for MoE, training pitfalls, and open research directions.\n",
    "\n",
    "By the end you’ll understand *how* MoE squeezes more intelligence out of the same GPU budget and *why* it is becoming a key ingredient of next-generation LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c93a4-e500-4480-a9fe-16e50791a9f6",
   "metadata": {
    "id": "b88c93a4-e500-4480-a9fe-16e50791a9f6"
   },
   "source": [
    "## 1 Background:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed0608-413e-48b7-a512-3b3cc884ca26",
   "metadata": {},
   "source": [
    "### 1.1 Transformers\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/transformer.png\" width=\"300\"/>\n",
    "  <figcaption><b>Figure 1</b> – Transformer architecture.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1756a6a",
   "metadata": {},
   "source": [
    "#### Scaled Dot-Product Attention\n",
    "\n",
    "The **Scaled Dot-Product Attention** module computes attention scores between queries $Q$ and keys $K$, scales them by $\\tfrac{1}{\\sqrt{d_k}}$ to stabilize gradients, applies an optional mask (e.g. to ignore padding or future tokens), and then uses softmax-normalized scores to weight the values $V$. This core operation lets each position in the sequence gather information from all other positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe4dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention:\n",
    "      Attention(Q, K, V) = softmax(Q K^T / sqrt(d_k)) V\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Q, K, V: (batch, n_heads, seq_len, d_k)\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b493c5",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "\n",
    "The **Multi-Head Attention** module runs several scaled-dot products in parallel (“heads”), allowing the model to jointly attend to information from different representation subspaces. It projects the input into multiple query/key/value spaces, applies attention in each head, concatenates the results, then applies a final linear projection plus residual & layer-norm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1904e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head wrapper around ScaledDotProductAttention.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Linear projections\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, D = x.size()\n",
    "        # project & reshape to (batch, n_heads, seq_len, d_k)\n",
    "        Q = self.W_Q(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_K(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_V(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        context, attn_weights = self.attn(Q, K, V, mask=mask)\n",
    "\n",
    "        # concat & project back\n",
    "        context = context.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        out = self.dropout(self.fc_out(context))\n",
    "        out = self.layer_norm(out + x)\n",
    "        return out, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e75c11",
   "metadata": {},
   "source": [
    "#### Position-wise Feed-Forward Network\n",
    "\n",
    "The **Position-wise Feed-Forward** block applies a two-layer MLP to each position independently:\n",
    "$$\n",
    "\\text{FFN}(x) = \\mathrm{Dropout}\\bigl(W_2\\,\\mathrm{ReLU}(W_1 x + b_1) + b_2\\bigr)\n",
    "$$\n",
    "A residual connection and layer normalization follow to stabilize training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-layer feed-forward network applied per position.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return self.layer_norm(out + x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4168c4",
   "metadata": {},
   "source": [
    "#### Transformer Encoder Layer\n",
    "\n",
    "A single encoder layer consists of:\n",
    "1. **Self-Attention** (Multi-Head Attention over the input sequence)  \n",
    "2. **Feed-Forward** (Position-wise MLP)  \n",
    "Each sublayer uses its own residual connection and layer normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96404a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ffn       = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        x, _ = self.self_attn(x, mask=src_mask)\n",
    "        x     = self.ffn(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbceb1e0",
   "metadata": {},
   "source": [
    "#### Transformer Decoder Layer\n",
    "\n",
    "Each decoder layer has three sublayers:\n",
    "1. **Masked Self-Attention** (prevents future-token attention)  \n",
    "2. **Cross-Attention** (attends to encoder output)  \n",
    "3. **Feed-Forward**  \n",
    "Residual connections and layer norms are applied around each sublayer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730fb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn  = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ffn        = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, tgt_mask=None, memory_mask=None):\n",
    "        x, _ = self.self_attn(x, mask=tgt_mask)\n",
    "        x, _ = self.cross_attn(x=x, mask=memory_mask, x_kv=enc_out)\n",
    "        x    = self.ffn(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b800c405",
   "metadata": {},
   "source": [
    "#### Full Transformer Model\n",
    "\n",
    "The top-level `Transformer` class orchestrates:\n",
    "1. **Embedding + Positional Encoding** of source and target tokens  \n",
    "2. **Encoder Stack** of $N$ encoder layers  \n",
    "3. **Decoder Stack** of $M$ decoder layers  \n",
    "4. **Final Linear** projection to vocabulary logits for next-token prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ce856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 src_vocab_size, tgt_vocab_size,\n",
    "                 d_model=512, n_heads=8, d_ff=2048, \n",
    "                 num_enc_layers=6, num_dec_layers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # embeddings + positional encoding\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        # encoder & decoder stacks\n",
    "        self.enc_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(num_enc_layers)\n",
    "        ])\n",
    "        self.dec_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(num_dec_layers)\n",
    "        ])\n",
    "\n",
    "        # output projection\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def encode(self, src, src_mask=None):\n",
    "        x = self.positional_encoding(self.src_tok_emb(src))\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, src_mask=src_mask)\n",
    "        return x\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        x = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x, enc_out=memory, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
    "        return x\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        memory = self.encode(src, src_mask)\n",
    "        out    = self.decode(tgt, memory, tgt_mask, memory_mask)\n",
    "        return self.fc_out(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50102d13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16ecc2-ad9d-4f1c-9bfe-788abe1f2d04",
   "metadata": {
    "id": "4b16ecc2-ad9d-4f1c-9bfe-788abe1f2d04"
   },
   "source": [
    "### 1.2 Mixture-of-Experts (MoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d106ada-a163-49f4-82af-ba3b2dd9bb2a",
   "metadata": {
    "id": "2d106ada-a163-49f4-82af-ba3b2dd9bb2a"
   },
   "source": [
    "Mixure-of-Experts (MoE) is a machine learning technique where multiple expert networks (learners) are used to divide a problem space into dedicated regions. Rather than a big one network that makes all the required tasks, it's splitted to many dedicated experts which each one of them makes a dedicated task.\n",
    "\n",
    "#### A Brief History of MoEs\n",
    "According to the following paper: https://huggingface.co/blog/moe <br>\n",
    "* The roots of MoEs come from the 1991 paper Adaptive Mixture of Local Experts.\n",
    "* The main idea was to have a supervised procedure for a system composed of separate networks, each handling a different subset of the training cases.\n",
    "* Each separate network, or expert, specializes in a different region of the input space.\n",
    "\n",
    "Between 2010-2015, two different research areas contributed to later MoE advancement (The years where Deep Neural Networks started being used):\n",
    "* Experts as components: In the traditional MoE setup, the whole system includes a gating network and multiple experts\n",
    "* MoEs as the whole model have been explored in ***SVMs***, ***Gaussian Processes***, and other methods\n",
    "* The work by Eigen, Ranzato, and Ilya from Google and NYU explored MoEs as components of deeper networks: This allows having MoEs as layers in a multilayer network, making it possible for the model to be both large and efficient simultaneously.\n",
    "* Conditional Computation: Traditional networks process all input data through every layer. In this period, Yoshua Bengio, a well known deep learning researcher from McGill university in Montréal researched approaches to dynamically activate or deactivate components based on the input token.\n",
    "* These works led to exploring a mixture of experts in the context of NLP.\n",
    "* Concretely, Shazeer et al. (2017, with “et al.” including Geoffrey Hinton and Jeff Dean, Google’s Chuck Norris) scaled this idea to a 137B LSTM (the de-facto NLP architecture back then, created by Schmidhuber) by introducing sparsity, allowing to keep very fast inference even at high scale.\n",
    "* This work focused on translation but faced many challenges, such as high communication costs and training instabilities.\n",
    "\n",
    "#### Some terms\n",
    "1. ***Expert***\n",
    "A small and specialized model which got trained for a particular area. It can be a neural network, decision tree, or other algorithm. In our case experts are small neural networks.\n",
    "\n",
    "3. ***A Mixture of Experts (MoE) model***\n",
    "A model that combines the predictions of multiple experts to solve complex problems.\n",
    "- Each expert is trained on a specific domain or task, and a \"gating network\" or \"router\" selects the most appropriate experts for a given input.\n",
    "\n",
    "3. ***\"gating network\" / \"router\"***\n",
    "A component (a tiny linear layer) in the large model that determines which experts should be activated for a particular input. It's also trained along with the experts\n",
    "\n",
    "#### What do we achieve from that ?\n",
    "The main benefit of the MoE architecture is that it enables large-scale models, even those comprising many billions of parameters, to reduce computation costs during pre-training and achieve faster performance during evaluation time.\n",
    "\n",
    "#### How does it work ?\n",
    "It reaches it's major benefit by selectively activating only the specific experts needed for a given task, rather than activating the entire neural network for every task.\n",
    "\n",
    "#### An illustration of a standard MoE network\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/moe.png?raw=1\" alt=\"Peer_Layer\" width=\"400\" height=\"400\">\n",
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/01_moe_layer.png?raw=1\" alt=\"Peer_Layer\" width=\"500\" height=\"600\">\n",
    "</figure>\n",
    "In the left image, the red experts are those who are active\n",
    "\n",
    "---\n",
    "\n",
    "####  Mixture-of-Experts implementation in popular LLMs:\n",
    "\n",
    "##### Mixtral 8×7B (Mistral)\n",
    "**Overview** – Mixtral augments a 7-billion-parameter base transformer with sparse MoE feed-forward blocks to reach **46.7 B total parameters** while keeping the *per-token* cost of roughly a 13 B dense model. It achieves this by letting each token consult only two of eight experts in every layer.\n",
    "\n",
    "* **MoE layout** – Every transformer block swaps its dense FFN for **8 identical SwiGLU experts**  \n",
    "  (hidden size = 14 336).  \n",
    "* **Router** – A learned linear gate $W_g \\in \\mathbb{R}^{d_{\\text{model}}\\times 8}$.\n",
    "\n",
    "  1. For each token $x$: logits $l = x W_g$\n",
    "  2. Keep the **top-2** logits, mask the rest to $-\\infty$\n",
    "  3. Weights $w = \\text{softmax}(l_{\\text{top-2}})$\n",
    "\n",
    "* **Capacity constraint**\n",
    "\n",
    "  $$\n",
    "    \\text{capacity} \\;=\\; \\Bigl\\lceil \\alpha \\,\\frac{T \\cdot K}{N}\\Bigr\\rceil,\n",
    "    \\qquad \\alpha \\approx 1.25\n",
    "  $$\n",
    "\n",
    "  where $T$=tokens in the batch, $K=2$ (top-k), $N=8$ experts.  \n",
    "  Overflow tokens are “zero-routed”.\n",
    "* **Auxiliary Switch loss** equalises both (i) probability mass and (ii) actual token counts per expert.\n",
    "* **Compute cost** – Only $2/8 = 25\\%$ of FFN compute runs per token, so inference costs ≈ 12.9 B-parameter dense model while training exploits the full **46.7 B** capacity.\n",
    "\n",
    "\n",
    "##### DeepSeek-MoE 16 B\n",
    "\n",
    "**Overview** – DeepSeek 16 B pushes specialisation further by *slicing* each FFN into many narrow **sub-experts**: 64 are sparsely routed and 2 are always on. This fine-grained design lets the gate compose highly specific mixtures without raising compute beyond that of a standard 16 B dense transformer.\n",
    "\n",
    "* **Layer composition** – Each MoE layer holds **64 routed sub-experts** + **2 always-on shared sub-experts**.\n",
    "* **Sub-expert** – Same two-layer SwiGLU as a full FFN expert but at **¼ hidden width**, so each is **4× cheaper**.\n",
    "* **Router (routed experts only)**\n",
    "\n",
    "  1. Score the 64 routed sub-experts with a linear gate.  \n",
    "  2. Keep the **top-6**, apply softmax → weights.  \n",
    "  3. Aggregate their weighted outputs.\n",
    "\n",
    "* **Final token output** = weighted sum of **6 routed sub-experts** **+** deterministic outputs of **2 shared sub-experts** ⇒ every token ultimately sees **8 sub-experts**.\n",
    "* **Capacity & balance** – Same ceiling formula as Mixtral (shared experts are exempt); auxiliary loss encourages even traffic across the 64 routed sub-experts.\n",
    "* **Why sub-experts?** Splitting the FFN into many narrow experts gives the router a richer palette of highly specialised functions while staying within the original FLOPs budget.  \n",
    "  Ablations show that removing just a few high-traffic sub-experts hurts perplexity far more than in classic MoE setups, signalling **stronger expert specialisation** and lower redundancy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e70a8c-89fa-4649-83ba-0e205befe346",
   "metadata": {
    "id": "b9e70a8c-89fa-4649-83ba-0e205befe346"
   },
   "source": [
    "## 2 Switch transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d7d88",
   "metadata": {},
   "source": [
    "### 2.1 Introduction:\n",
    "\n",
    "Large language models (LLMs) have achieved striking gains by growing from millions\n",
    "to billions of parameters—yet *dense* scaling makes **every** parameter participate\n",
    "in **every** forward-pass.  \n",
    "Compute (FLOPs), memory traffic, and wall-time therefore grow linearly with model\n",
    "size, and the trillion-parameter frontier strains even the largest clusters.\n",
    "\n",
    "**Mixture-of-Experts (MoE)** layers offer a different path: *conditional\n",
    "computation*.  \n",
    "A lightweight *gating network* selects **one** or **few** specialized “experts”\n",
    "(MLPs) per token, so only a *subset* of parameters is active each step.\n",
    "Switch Transformers (Fedus *et&nbsp;al.*, 2022) refine this idea to make it practical\n",
    "at unprecedented scale.\n",
    "\n",
    "#### Why previous MoE attempts struggled  \n",
    "\n",
    "| Bottleneck in earlier MoE work | What it means | Switch Transformer’s remedy |\n",
    "| :-- | :-- | :-- |\n",
    "| **Unstable top-k routing** (k > 1) | When every token is split across *k* experts (k = 2,4…), the soft mixture may starve some experts of gradient signal → divergence in very deep models. | **k = 1 “switch” routing**: each token is sent to exactly one expert chosen by `argmax` over gate logits. This keeps gradients intact and halves the routing tensor size. |\n",
    "| **Cross-device communication** | Prior systems sliced *one* expert across many GPUs/TPUs → every step required an All-to-All of hidden states. | **Expert-parallel layout**: each device *owns* one whole expert. Tokens are grouped by destination expert, transferred *once*, processed locally, then regrouped—minimising traffic. |\n",
    "| **Token imbalance (hot-spot experts)** | Popular tokens (e.g., punctuation) can overload a few experts, leaving others idle and blowing up memory. | **Auxiliary load-balancing loss**: penalises correlation between (i) fraction of tokens routed to expert *i* and (ii) gate probability mass on expert *i*.  <br>$$ L_{\\text{aux}} = \\alpha\\,N \\sum_{i=1}^{N} f_i\\,P_i \\quad\\text{(Eq.\\;4)} $$ |\n",
    "\n",
    "*Definitions*  \n",
    "* **Expert** – an independent feed-forward sub-network (here, a position-wise MLP).  \n",
    "* **Gating network** – a tiny linear layer that produces *N* logits per token.  \n",
    "* **Routing** – assigning tokens to experts based on gate probabilities.  \n",
    "* **FLOPs / token** – floating-point operations needed for one token’s forward-pass.\n",
    "\n",
    "---\n",
    "\n",
    "#### Research Questions\n",
    "\n",
    "This paper explicitly addresses several critical research questions:\n",
    "\n",
    "1. **Efficiency vs. Model Capacity:**  \n",
    "   *Does increasing model size through sparse routing (more experts) consistently improve language model performance (perplexity) without proportional computational cost?*\n",
    "\n",
    "2. **Training Stability:**  \n",
    "   *Can models at trillion-parameter scale be stably trained using lower-precision arithmetic (like bfloat16)?*\n",
    "\n",
    "3. **Downstream Generalization:**  \n",
    "   *Do improvements obtained during language modeling pre-training generalize to downstream tasks (e.g., QA, summarization)?*\n",
    "\n",
    "4. **Model Compression and Deployment:**  \n",
    "   *Is it feasible to distill sparse models into smaller, dense models while preserving performance improvements?*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff624e5",
   "metadata": {},
   "source": [
    "### 2.2 Model architecture overview  \n",
    "\n",
    "A Switch Transformer layer is identical to a standard Transformer block **except** that the\n",
    "dense Feed-Forward Network (FFN) is replaced by a **Switch-FFN** (sparse Mixture-of-Experts).  \n",
    "Figure 2 from the paper illustrates the encoder block with two example tokens (${x_1,x_2}$).  \n",
    "Only the shaded *Switch-FFN* (light-blue) differs from a dense model.  :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/figure2_switch_arch.png\" width=\"600\"/>\n",
    "  <figcaption><b>Figure 2</b> – Switch-FFN inside the Transformer block (Fedus et al., 2022).</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15977d0",
   "metadata": {},
   "source": [
    "#### 2.2.1 Switch Transformer Module\n",
    "\n",
    "The `SwitchTransformer` builds on the classic Transformer encoder by:\n",
    "\n",
    "1. **Embedding + Positional Encoding**  \n",
    "   - Converts token IDs into \\(d_{\\text{model}}\\)-dimensional vectors and adds sinusoidal (or learned) timing signals.\n",
    "\n",
    "2. **Layer Stack**  \n",
    "   - Clones a prototype `SwitchTransformerLayer` \\(L\\) times, enabling sparse, per-token expert routing at each depth.  \n",
    "   - Gathers routing statistics (`counts`, `load`, `dropped`, `max_p`) for every layer to support the auxiliary load-balancing loss.\n",
    "\n",
    "3. **Final Normalization**  \n",
    "   - Applies a top-level `LayerNorm` to stabilize outputs across the full depth.\n",
    "\n",
    "This architecture maintains the Transformer’s depth and self-attention benefits while drastically increasing capacity via \\(N\\) experts, all at constant computational cost per token.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b50fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class SwitchTransformer(Module):\n",
    "    \"\"\"\n",
    "    Stacks multiple SwitchTransformerLayer’s into a full encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, *,\n",
    "                 src_vocab_size: int, d_model: int, n_heads: int,\n",
    "                 d_ff: int, n_layers: int,\n",
    "                 capacity_factor: float, drop_tokens: bool,\n",
    "                 is_scale_prob: bool, n_experts: int,\n",
    "                 max_seq_len: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        # Token embedding + positional encoding\n",
    "        self.token_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_enc   = PositionalEncoding(d_model, dropout_prob, max_seq_len)\n",
    "\n",
    "        # Stack of SwitchTransformerLayers\n",
    "        layer = SwitchTransformerLayer(\n",
    "            d_model=d_model, n_heads=n_heads, d_ff=d_ff,\n",
    "            capacity_factor=capacity_factor, drop_tokens=drop_tokens,\n",
    "            is_scale_prob=is_scale_prob, n_experts=n_experts,\n",
    "            dropout_prob=dropout_prob\n",
    "        )\n",
    "        self.layers    = clone_module_list(layer, n_layers)\n",
    "        self.norm      = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None):\n",
    "        # 1) Embed & add positional encoding\n",
    "        x = self.token_emb(src) * math.sqrt(self.token_emb.embedding_dim)\n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        # 2) Apply each SwitchTransformerLayer\n",
    "        all_stats = []\n",
    "        for layer in self.layers:\n",
    "            x, counts, load, dropped, max_p = layer(x, mask=src_mask)\n",
    "            all_stats.append({\n",
    "                'counts': counts,\n",
    "                'load': load,\n",
    "                'dropped': dropped,\n",
    "                'max_p': max_p\n",
    "            })\n",
    "\n",
    "        # 3) Final layer normalization\n",
    "        x = self.norm(x)\n",
    "        return x, all_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7069f0",
   "metadata": {},
   "source": [
    "#### 2.2.2 Switch TransformerLayer Module\n",
    "\n",
    "This layer mirrors a standard Transformer block but replaces the fixed-position FFN with a sparse “SwitchFeedForward” mixture-of-experts:\n",
    "\n",
    "1. **Self-Attention Sub-Layer**  \n",
    "   - Applies pre-norm, multi-head self-attention (`MultiHeadAttention`) to allow tokens to mix contextual information.  \n",
    "   - Adds a residual connection and dropout.\n",
    "\n",
    "2. **Mixture-of-Experts Feed-Forward Sub-Layer**  \n",
    "   - Applies pre-norm, then routes each token through exactly one of $N$ expert FFNs via `SwitchFeedForward`.  \n",
    "   - Collects routing statistics (`counts`, `load`, `dropped`, `max_p`) for auxiliary losses.  \n",
    "   - Adds a residual connection and dropout.\n",
    "\n",
    "Each sublayer uses its own `LayerNorm` and the same `dropout_prob` for stability and regularization.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class SwitchTransformerLayer(Module):\n",
    "    \"\"\"\n",
    "    A single Transformer block using SwitchFeedForward instead of a fixed FFN.\n",
    "    \"\"\"\n",
    "    def __init__(self, *, d_model: int, n_heads: int, d_ff: int,\n",
    "                 capacity_factor: float, drop_tokens: bool,\n",
    "                 is_scale_prob: bool, n_experts: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        # 1) Multi-head self-attention sublayer\n",
    "        self.attn       = MultiHeadAttention(d_model, n_heads, dropout_prob)\n",
    "        # 2) Mixture-of-experts feed-forward sublayer\n",
    "        base_ffn        = PositionwiseFeedForward(d_model, d_ff, dropout_prob)\n",
    "        self.feed_forward = SwitchFeedForward(\n",
    "            capacity_factor=capacity_factor,\n",
    "            drop_tokens=drop_tokens,\n",
    "            is_scale_prob=is_scale_prob,\n",
    "            n_experts=n_experts,\n",
    "            expert=base_ffn,\n",
    "            d_model=d_model\n",
    "        )\n",
    "        # Layer norms & dropout\n",
    "        self.norm1      = nn.LayerNorm(d_model)\n",
    "        self.norm2      = nn.LayerNorm(d_model)\n",
    "        self.dropout    = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None):\n",
    "        # 1) Pre-norm + self-attention\n",
    "        z1, _      = self.attn(self.norm1(x), mask=mask)\n",
    "        x          = x + self.dropout(z1)\n",
    "\n",
    "        # 2) Pre-norm + switch-FFN\n",
    "        z2, counts, load, dropped, max_p = self.feed_forward(self.norm2(x))\n",
    "        x          = x + self.dropout(z2)\n",
    "\n",
    "        return x, counts, load, dropped, max_p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e57cd",
   "metadata": {},
   "source": [
    "#### 2.2.3 Switch FeedForward Module\n",
    "Initializes the mixture-of-experts block by:\n",
    "- Replicating the base feed-forward network into `n_experts` separate experts.\n",
    "- Defining a lightweight router (`self.switch`) that maps each token embedding of size `d_model` to unnormalized logits for each expert.\n",
    "- Preparing to convert those logits into a probability distribution with `Softmax`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7765e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class SwitchFeedForward(Module):\n",
    "    def __init__(self, *, capacity_factor: float, drop_tokens: bool,\n",
    "                 is_scale_prob: bool, n_experts: int,\n",
    "                 expert: FeedForward, d_model: int):\n",
    "        super().__init__()\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.drop_tokens     = drop_tokens\n",
    "        self.is_scale_prob   = is_scale_prob\n",
    "        self.n_experts       = n_experts\n",
    "        # 1) Create N independent copies of the base FFN\n",
    "        self.experts = clone_module_list(expert, n_experts)\n",
    "        # 2) Router: a linear map to produce logits for each expert\n",
    "        self.switch  = nn.Linear(d_model, n_experts)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4699f1",
   "metadata": {},
   "source": [
    "##### Forward funciton\n",
    "\n",
    "This function implements the forward pass of the `SwitchFeedForward` mixture-of-experts layer. It accepts an input tensor `x` of shape `[seq_len, batch, d_model]`, where:\n",
    "\n",
    "- `seq_len` is the sequence length  \n",
    "- `batch` is the batch size  \n",
    "- `d_model` is the model’s hidden dimension  \n",
    "\n",
    "Inside, each token is separated out for per-token routing across experts; the method ultimately returns both the transformed output and auxiliary routing statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6219c56",
   "metadata": {},
   "source": [
    "##### Flatten Inputs\n",
    "\n",
    "Reshapes input from shape `[seq_len, batch, d_model]` into `[T, d_model]` with `T = seq_len × batch`, so that each token is routed independently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf9d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "    def forward(self, x: Tensor):\n",
    "        seq_len, batch, d_model = x.shape\n",
    "        # Collapse sequence and batch dims for per-token routing\n",
    "        flat_x = x.view(-1, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a3d315",
   "metadata": {},
   "source": [
    "##### Compute Raw Routing Logits\n",
    "\n",
    "Each token embedding `x` is linearly projected to produce unnormalized scores:\n",
    "$$\n",
    "h_i(x) = \\bigl[W_{\\text{switch}}\\,x + b\\bigr]_i,\\quad i=1,\\dots,N.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317314b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "        logits = self.switch(flat_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bffe4ac",
   "metadata": {},
   "source": [
    "##### Normalize into Routing Probabilities\n",
    "\n",
    "Applies softmax to the logits to yield a distribution over experts:\n",
    "$$\n",
    "p_i(x) = \\frac{\\exp\\bigl(h_i(x)\\bigr)}{\\sum_{j=1}^N \\exp\\bigl(h_j(x)\\bigr)},\\quad \\sum_i p_i(x)=1.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200a860",
   "metadata": {},
   "source": [
    " %%\n",
    "        route_prob = self.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05563ed1",
   "metadata": {},
   "source": [
    "##### Pick Top-1 Expert\n",
    "\n",
    "Routes each token to the expert with maximum probability:\n",
    "$$\n",
    "i^*(x) = \\arg\\max_i\\,p_i(x),\\qquad p_{i^*}(x)=\\max_i p_i(x).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "        prob_max, routes = torch.max(route_prob, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f5376",
   "metadata": {},
   "source": [
    "##### Compute Expert Capacity\n",
    "\n",
    "Determines the per-expert capacity:\n",
    "$$\n",
    "C = \\Bigl\\lfloor \\alpha\\times\\frac{T}{N}\\Bigr\\rfloor,\n",
    "$$\n",
    "where $T$ is total tokens, $N$ experts, and $\\alpha$ is `capacity_factor`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "        capacity = int(self.capacity_factor * flat_x.size(0) / self.n_experts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc249f6b",
   "metadata": {},
   "source": [
    "##### Enforce Capacity & Optionally Drop\n",
    "\n",
    "1. **Counts:** number of tokens routed to each expert.  \n",
    "2. If `drop_tokens=True`, any expert receiving more than $C$ tokens will randomly drop the excess; dropped tokens bypass later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59234007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "        counts  = torch.tensor([len((routes==i).nonzero()) for i in range(self.n_experts)])\n",
    "        dropped = []\n",
    "        if self.drop_tokens:\n",
    "            for i in range(self.n_experts):\n",
    "                idxs = (routes==i).nonzero(as_tuple=True)[0]\n",
    "                if idxs.numel() > capacity:\n",
    "                    perm = idxs[torch.randperm(idxs.numel())]\n",
    "                    dropped.append(perm[capacity:])\n",
    "                    idxs = perm[:capacity]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fdee90",
   "metadata": {},
   "source": [
    "##### Dispatch Tokens to Experts\n",
    "Applies each expert $E_i$ only to its assigned subset $\\mathcal{I}_i$:\n",
    "$$\n",
    "y_i = E_i\\bigl(x_{\\mathcal{I}_i}\\bigr).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81287ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "        final_out = flat_x.new_zeros(flat_x.shape)\n",
    "        for i in range(self.n_experts):\n",
    "            idxs = (routes==i).nonzero(as_tuple=True)[0]\n",
    "            if idxs.numel() > 0:\n",
    "                out_i = self.experts[i](flat_x[idxs])\n",
    "                final_out[idxs] = out_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d7fbe2",
   "metadata": {},
   "source": [
    "##### Handle Dropped Tokens\n",
    "\n",
    "Tokens dropped due to capacity limits are passed through unchanged:\n",
    "$$\n",
    "y_{\\mathrm{bypass}}(x) = x.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855dfe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "        if dropped:\n",
    "            all_dropped = torch.cat(dropped)\n",
    "            final_out[all_dropped] = flat_x[all_dropped]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856285dc",
   "metadata": {},
   "source": [
    "#####  Scale by Gate Value\n",
    "\n",
    "Modulates each token’s expert output by its gate value:\n",
    "$$\n",
    "\\tilde y(x) = p_{i^*}(x)\\,y_{i^*}(x).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ced04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "        if self.is_scale_prob:\n",
    "            final_out = final_out * prob_max.unsqueeze(-1)\n",
    "        else:\n",
    "            final_out = final_out * (prob_max/prob_max.detach()).unsqueeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81778e",
   "metadata": {},
   "source": [
    "#####  Restore Shape & Return\n",
    "\n",
    "Reshapes the result back to `[seq_len, batch, d_model]` and returns auxiliary statistics for load-balancing loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff54e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "        output = final_out.view(seq_len, batch, d_model)\n",
    "        return output, counts, route_prob.sum(0), sum(len(d) for d in dropped), prob_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2691aa1d",
   "metadata": {},
   "source": [
    "####  2.2.4 **Capacity and Hard Limit**\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/figure3_capacity_dynamics.png\" width=\"650\"/>\n",
    "  <figcaption><b>Figure 3 — Token-routing dynamics under two capacity factors.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "**What the diagram shows**\n",
    "\n",
    "*Left panel (capacity 1.0)*  \n",
    "* 12 tokens must be routed across three experts (rows).  \n",
    "* Each expert’s capacity is exactly the ideal load \\(B/N = 4\\) tokens.  \n",
    "* Because many tokens happen to share the same favourite expert, that expert\n",
    "  overflows — the dotted red boxes show the **dropped** tokens that will bypass\n",
    "  this layer.\n",
    "\n",
    "*Right panel (capacity 1.5)*  \n",
    "* The capacity per expert is now \\(4 \\times 1.5 = 6\\) tokens, giving 50 % slack.  \n",
    "* All 12 tokens fit; no red overflow boxes, but a few **empty white slots** indicate\n",
    "  wasted compute/communication.\n",
    "\n",
    "**Take-away** A small slack margin (the paper standardises on 1.25) almost\n",
    "eliminates overflows yet keeps extra FLOPs and bandwidth modest.  Capacity is\n",
    "therefore the runtime *circuit-breaker* that guarantees fixed memory and latency\n",
    "even when the router’s token-to-expert distribution is skewed.\n",
    "\n",
    "**Why does capacity matter?**\n",
    "\n",
    "Routing decisions can lead to unbalanced token assignments, with some experts overloaded while others remain underutilized. Without mitigation, overloaded experts would cause out-of-memory errors, degraded performance, and unpredictable latency.\n",
    "\n",
    "Switch Transformer addresses this via a **hard capacity limit** on each expert:\n",
    "\n",
    "$$\n",
    "\\text{capacity per expert} = \\left\\lceil \\frac{B}{N} \\times \\text{capacity\\_factor} \\right\\rceil\n",
    "$$\n",
    "\n",
    "- $ B $: Total number of tokens in the micro-batch\n",
    "- $ N $: Number of experts\n",
    "- $\\text{capacity\\_factor}$: Typically set to $1.25$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb742a",
   "metadata": {},
   "source": [
    "\n",
    "####  2.2.5 **Load-Balancing Losses**\n",
    "\n",
    "Early attempts at Mixture-of-Experts models encountered a critical problem known as **expert collapse**, where a small number of experts dominated token assignments, starving others of training signal. Switch Transformers introduced two auxiliary losses to prevent collapse and encourage balanced expert usage:\n",
    "\n",
    "##### 1. **Auxiliary Load-Balancing Loss**\n",
    "\n",
    "$$\n",
    "L_{\\text{aux}} = \\alpha N \\sum_{i=1}^{N} f_i P_i,\\quad\n",
    "f_i = \\frac{\\text{tokens routed to expert } i}{\\text{batch size}},\\quad\n",
    "P_i = \\text{average gate probability for expert } i\n",
    "$$\n",
    "\n",
    "Encourages even token distribution by penalizing high correlation between an expert’s frequency of selection and router confidence.\n",
    "\n",
    "You can think of $\\mathbf f^\\top \\mathbf P$ as the **expected confidence** the router has in the experts it actually chooses.  \n",
    "- If the router always picks highly‐confident experts (i.e.\\ $\\mathbf P$ is very “peaked”), then $\\mathbf f^\\top \\mathbf P$ will be high.  \n",
    "- By **penalizing** $\\sum_{i=1}^N f_i\\,P_i = \\mathbf f^\\top \\mathbf P$, we force the router to **spread its confidence** more evenly across all experts.  \n",
    "- As a result, the actual assignments $\\mathbf f$ become more balanced, improving expert utilization and reducing hardware bottlenecks.\n",
    "\n",
    "\n",
    "##### 2. **Z-Loss (Logit Regularization)**\n",
    "\n",
    "\n",
    "$$\n",
    "L_z = \\beta \\sum_{\\text{tokens}}\\sum_{i}(h_i(token_i) - \\text{stop\\_grad}(token_i))^2\n",
    "$$\n",
    "\n",
    "**Intuition:**  \n",
    "- Encourages router logits $h_i$ to remain small in magnitude, preventing overly confident selections (extremely large or small logits can destabilize softmax distributions).\n",
    "- Maintains stable gradient flow and well-conditioned router softmax.\n",
    "\n",
    "\n",
    "1. **Definition of** $\\mathrm{stop\\_grad}(x)$:  \n",
    "   $$\n",
    "     \\mathrm{stop\\_grad}(x)\\;=\\;x\n",
    "     \\quad\\text{(forward value)}\\,,\\qquad\n",
    "     \\frac{\\partial\\,\\mathrm{stop\\_grad}(x)}{\\partial x}=0\n",
    "     \\quad\\text{(blocks gradients)}\n",
    "   $$\n",
    "\n",
    "2. **Corrected Z-Loss** for a target logit $h_i(x)$:  \n",
    "   $$\n",
    "     \\mathcal{L}_Z\n",
    "     \\;=\\;\\beta\\;\\bigl(h_i(x)\\;-\\;\\mathrm{stop\\_grad}\\bigl[\\log\\!\\sum_{j=1}^N e^{h_j(x)}\\bigr]\\bigr)^{2}.\n",
    "   $$\n",
    "\n",
    "3. **Relation to log-softmax**:  \n",
    "   $$\n",
    "     h_i(x)-\\log\\!\\sum_{j=1}^N e^{h_j(x)}\n",
    "     =\\log\\!\\bigl(e^{h_i(x)}\\bigr)\\;-\\;\\log\\!\\sum_{j=1}^N e^{h_j(x)}\n",
    "     =\\log p_i(x),\n",
    "   $$  \n",
    "   so this difference directly measures the **log-probability** of expert $i$ under the softmax.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a96b60",
   "metadata": {},
   "source": [
    "### 2.3 Experiments & Results\n",
    "#### Baseline Models Used for Comparison  \n",
    "\n",
    "The authors selected **four families** of comparison models, each serving a distinct purpose.\n",
    "\n",
    "####   Dense T5 Series – \n",
    "**T5** stands for **“Text-to-Text Transfer Transformer.”**  \n",
    "Released by Google in late 2019, it introduced a simple yet powerful idea: *cast every NLP task—translation, summarisation, QA, sentiment, …—as feeding one piece of text in and predicting another piece of text out.*  \n",
    "This unification plus large-scale span-corruption pre-training on the **C4** web corpus produced a strong encoder–decoder baseline.\n",
    "\n",
    "**Why the authors picked T5:**\n",
    "\n",
    "1. **Like-for-like objective and codebase** – eliminates spurious gains from task formulation or optimiser tweaks.  \n",
    "2. **Widely reported benchmarks** – GLUE / SuperGLUE / SQuAD scores for T5 are standard yard-sticks, so improvements are easy to contextualise.  \n",
    "3. **Scales up smoothly** – letting the paper test whether sparse scaling beats a dense model simply made *bigger* (e.g., T5-Large or T5-XXL).\n",
    "\n",
    "Thus, throughout the experiments T5 provides a **clean, well-understood dense baseline** against which the efficiency and quality of the Switch (sparse) approach can be judged.\n",
    "\n",
    "\n",
    "| Model | Params | FLOPs / token | Why chosen |\n",
    "|-------|--------|--------------|-----------|\n",
    "| **T5-Base** | 223 M | 1 × (reference) | Same size class as 2-expert Switch; establishes a dense baseline that already fits on a single TPU/GPU. |\n",
    "| **T5-Large** | 739 M | 3.5 × Switch-Base | Represents a “scale-up dense” strategy within the same architecture and codebase. |\n",
    "| **T5-XXL** | 11 B | 6.3 T per seq | State-of-the-art dense model at publication time; tests whether sparse can outpace *very* large dense models under the same cluster budget. |\n",
    "\n",
    "*Rationale* – All T5 variants share the *exact* training objective, tokenizer, and optimizer code. That isolates the effect of **conditional vs dense compute**.\n",
    "\n",
    "#####   MoE Transformer (Top-2 Routing) – \n",
    "\n",
    "| Variant | Experts | Routing | Why chosen |\n",
    "|---------|---------|---------|-----------|\n",
    "| **MoE-Transformer (Shazeer et al.)** | 128 | top-2 | Prevailing MoE design before Switch; higher FLOPs because two experts fire per token. |\n",
    "\n",
    "*Rationale* – Validates whether **single-expert** routing is genuinely more efficient/stable than the established top-k approach.\n",
    "\n",
    "##### Why these baselines are fair  \n",
    "\n",
    "* **Same tokenizer and data** → eliminates corpus effects.  \n",
    "* **Same optimizer hyper-params** (where feasible) → isolates architectural difference.  \n",
    "* **FLOP-matched pairs** (Switch-Base vs T5-Base, Switch-Large vs T5-Large) → asks:  \n",
    "  > *“Given the **same compute budget**, which architecture learns faster / better?”*  \n",
    "* **Higher-FLOP dense models** (T5-Large, -XXL) → test the critique  \n",
    "  > *“Just spend more FLOPs on dense; why bother with sparsity?”*  \n",
    "* **Legacy MoE top-2** → ensures the improvement isn’t merely “MoE vs dense” but due to the **Switch simplification**.\n",
    "\n",
    "Using this spectrum of baselines, the paper demonstrates that Switch Transformers outperform:\n",
    "\n",
    "1. **Compute-matched dense** models (fair efficiency test),  \n",
    "2. **Heavier dense** models (efficiency-vs-quality frontier), and  \n",
    "3. **Previous sparse** architectures (methodological advance).\n",
    "\n",
    "This comprehensive baseline suite strengthens the claim that **conditional compute via single-expert routing is a superior scaling path**.\n",
    "\n",
    "####  Scaling Properties \n",
    "The paper’s **Scaling Properties** section asks:  \n",
    "> *“If we keep FLOPs / token roughly constant, how far can we improve quality by adding more experts (i.e., more parameters)?”*  \n",
    "\n",
    "To answer, the authors run three tightly-controlled experiments.\n",
    "\n",
    "#####  Step-Basis Scaling\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/figure_4_step_basis_scaling.png\" width=\"600\"/>\n",
    "  <figcaption><b>Figure: Scaling Switch Transformer (perplexity vs. training steps and wall-clock).</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Setup** | Pre-train **Switch-Base** models with 2 → 256 experts (223 M → 14.7 B parameters) **for a fixed 100 k steps**. FLOPs/token stay constant because each token still activates one expert. |\n",
    "| **Purpose** | Is parameter-only scaling (via experts) a free win when compute is fixed? |\n",
    "| **Main results** | Perplexity **drops monotonically** as experts double. The 64-expert model matches T5-Base quality **7.5 × sooner** in steps. |\n",
    "| **Interpretation** | Extra capacity (parameters) is effectively used even though compute is unchanged. Sparse routing is therefore a *new scaling axis* orthogonal to FLOPs. |\n",
    "\n",
    "#####  Time-Basis Scaling\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/figure_5_time_basis_scaling.png\" width=\"300\"/>\n",
    "  <figcaption><b>Figure: Scaling Switch Transformer (perplexity vs. training time and wall-clock).</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Setup** | Measure **wall-clock minutes** to reach target perplexities on identical TPU pods. Same model family as above. |\n",
    "| **Purpose** | Extra experts add routing overhead (softmax, All-to-All). Do they erase the step advantage? |\n",
    "| **Main results** | Sparse models *still* win: 64-expert Switch reaches T5-Base quality in **≈ 140 min vs 350 min** (≈ 2.5× faster). |\n",
    "| **Conclusion** | Routing + communication overhead is small relative to the gains from parameter scaling. Sparse models give **real-time savings**, not just step savings. |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#####  Sparse vs. “Just Make the Dense Model Bigger”\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/figure_6_parameters_basis_scaling.png\" width=\"600\"/>\n",
    "  <figcaption><b>Figure: Sample Efficiency Switch Transformer VS T5 variants.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Setup** | Compare **Switch-Base (64 e)** against **T5-Large** which spends **3.5× more FLOPs/token** than Switch-Base. |\n",
    "| **Purpose** | Critics could argue “dense scaling already works—just spend more FLOPs.” |\n",
    "| **Main results** | Despite T5-Large’s heavier compute, Switch-Base is **2.5× faster** to the same perplexity and *still* ends lower. |\n",
    "| **Conclusion** | Conditional computation **dominates** naive dense scaling in the speed/quality trade-off. You can’t buy the same improvement just by burning more FLOPs per token. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759c905",
   "metadata": {},
   "source": [
    "####  Down-Stream Experiments\n",
    "\n",
    "The authors performed five focused studies to verify that the pre-training gains of Switch Transformers **transfer** to real tasks and to understand how best to fine-tune, regularise, and deploy very large sparse models.\n",
    "\n",
    "\n",
    "#####  Fine-Tuning Benchmark Suite\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/table_5_fine_tuning_results.png\" width=\"400\"/>\n",
    "  <figcaption><b>Table: Fine-tuning results. T5 baselines VS Switch models across\n",
    "  a diverse set of natural language test.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "- **GLUE —** A bundle of nine sentence‐level and sentence-pair evaluations (sentiment, paraphrase, natural-language inference, etc.) that together gauge broad language understanding in English.\n",
    "\n",
    "- **SuperGLUE —** A harder successor to GLUE featuring multi-sentence reasoning tasks such as BoolQ, ReCoRD, and WSC; designed to test deeper compositional reasoning and commonsense.\n",
    "\n",
    "- **SQuAD v1.1 —** Reading-comprehension question answering on Wikipedia passages; checks the model’s ability to locate and extract exact answer spans from context.\n",
    "\n",
    "- **XSum —** Single-sentence abstractive news summarisation; evaluates whether the system can condense an article into one concise, fluent sentence while preserving key facts.\n",
    "\n",
    "- **Winogrande —** Commonsense pronoun-resolution puzzles; measures the model’s grasp of implicit world knowledge needed to resolve ambiguous references.\n",
    "\n",
    "- **TriviaQA (closed-book) —** Open-domain factoid QA answered without external documents; probes how much factual knowledge is stored internally in the model’s parameters.\n",
    "\n",
    "- **ANLI —** Adversarial natural-language inference collected via model-in-the-loop annotation; assesses robustness to deliberately tricky NLI examples.\n",
    "\n",
    "- **ARC (Easy & Challenge) —** Multiple-choice grade-school science-exam questions; tests logical reasoning over short factual statements rather than surface pattern matching.\n",
    "\n",
    "\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Setup** | Fine-tuned FLOP-matched pairs: **Switch-Base (7 B) vs T5-Base (0.2 B)** and **Switch-Large (26 B) vs T5-Large (0.7 B)** on GLUE, SuperGLUE, SQuAD, XSum, Winogrande, TriviaQA, ANLI, ARC. Dropout: 0.1 non-expert, 0.4 expert; 100 k training steps. |\n",
    "| **Purpose** | Confirm that sparse pre-training advantages appear on diverse NLU, QA, and summarisation tasks. |\n",
    "| **Main results** | • **+4.4 pp SuperGLUE** (Base) and +2 pp (Large).<br>• Closed-book TriviaQA +6 pp.<br>• Gains on Winogrande, XSum; mixed on ARC. |\n",
    "| **Conclusion** | Pre-training gains **transfer broadly**; sparse capacity especially helps knowledge-heavy tasks. |\n",
    "\n",
    "####  Distillation for Deployment\n",
    "\n",
    "##### How knowledge distillation works in general  \n",
    "\n",
    "1. **Run the teacher** – pass each input through the (large) teacher model  \n",
    "   * obtain either the **logits** $z^{(T)}$ or the softened probabilities  \n",
    "     $\\sigma(z^{(T)}/T)$ at temperature $T>1$.\n",
    "\n",
    "2. **Run the student** – pass the *same* input through the small model  \n",
    "   to produce logits $z^{(S)}$.\n",
    "\n",
    "3. **Blend two losses**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}\n",
    "  = \\lambda \\; \\underbrace{\\mathrm{KL}\\!\\bigl[\\sigma(z^{(T)}/T)\\;\\|\\;\\sigma(z^{(S)}/T)\\bigr]}_{\\text{soft-target loss}}\n",
    "  + (1-\\lambda)\\; \\underbrace{\\mathrm{CE}\\!\\bigl[y,\\;\\sigma(z^{(S)})\\bigr]}_{\\text{hard-target loss}}\n",
    "$$\n",
    "\n",
    "* $y$ – ground-truth labels  \n",
    "* $\\sigma$ – softmax  \n",
    "* $\\lambda$ – weight that trades off “mimic the teacher” vs. “fit the labels”  \n",
    "* Back-propagate **only through the student**; the teacher is frozen.\n",
    "\n",
    "4. **Optimise the student** until validation perplexity or task metric plateaus.  \n",
    "   The student thus learns a compressed approximation of the teacher’s behaviour\n",
    "   while still respecting the original task labels.\n",
    "\n",
    "\n",
    "##### Switch-Transformer distillation  \n",
    "\n",
    "* **Teacher models** – sparse Switch-Base variants  \n",
    "  * 3.8 B , 7.4 B , 14.7 B parameters (64 or 128 experts)  \n",
    "  * already pre-trained on C4; one run fine-tuned on SuperGLUE.\n",
    "\n",
    "* **Student model** – dense T5-Base, 223 M parameters.\n",
    "\n",
    "* **Weight initialisation** – copy all **non-expert weights** (embeddings, attention,\n",
    "  residual projections) from teacher to student; randomly init the rest.\n",
    "\n",
    "* **Soft / hard mix** –  \n",
    "  $\\lambda = 0.25$ for the soft-target KL term, $1-\\lambda = 0.75$ for the standard cross-entropy.\n",
    "\n",
    "* **Temperature** – $T = 2.0$ to soften the teacher’s probability distribution.\n",
    "\n",
    "* **Training data & length** –  \n",
    "  * C4 span-corruption for language modelling distillation (150 k steps).  \n",
    "  * SuperGLUE labelled set for task-specific distillation (same step budget).\n",
    "\n",
    "* **Optimiser & schedule** – identical Adafactor settings used in pre-training; no extra tricks.\n",
    "\n",
    "* **Outcome** – student keeps **≈30 %** of the teacher’s quality gain while shrinking\n",
    "  model size by **95–99 %**, demonstrating a deployable path for Switch-Transformer\n",
    "  knowledge.\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/table_6_distillition.png\" width=\"400\"/>\n",
    "  <figcaption><b>Table: Fine-tuning results. T5 baselines VS Switch models across\n",
    "  a diverse set of natural language test.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **What was done** | Distilled sparse **Switch-Base 3.8 B / 7.4 B / 14.7 B** teachers into a 223 M dense T5-Base student. Tricks: initialise student with teacher’s non-expert weights + 0.25 × soft-loss + 0.75 × hard-loss. |\n",
    "| **Why** | Massive trillion-parameter models are hard to deploy; distillation offers a lighter alternative. |\n",
    "| **Main results** | • **≈ 30 % of teacher gain retained** at 95–99 % compression.<br>• Fine-tuned SuperGLUE distillation keeps 30 % gain on a 97 % compressed model. |\n",
    "| **Interpretation** | Distillation provides a **practical path** from huge sparse teachers to deployable dense students while preserving a meaningful slice of quality improvement. |\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b9eb59",
   "metadata": {},
   "source": [
    "### 2.4 Switch Extensions: MixLoRA & MoE-Mamba\n",
    "\n",
    "#### MixLoRA: LoRA-Based Sparse MoE (Li et al., 2024)\n",
    "\n",
    "**At a glance**  \n",
    "A parameter-efficient sparse MoE that injects LoRA adapters as experts into a frozen backbone for multi-task fine-tuning on modest GPUs.\n",
    "\n",
    "**What is LoRA?**  \n",
    "LoRA (Low-Rank Adaptation) freezes a large weight matrix $W\\in\\mathbb{R}^{d\\times d}$ and learns two much smaller matrices $A\\in\\mathbb{R}^{d\\times r}$ and $B\\in\\mathbb{R}^{r\\times d}$ with $r\\ll d$, such that\n",
    "$$\n",
    "W' = W + B\\,A.\n",
    "$$\n",
    "\n",
    "**Switch-inspired element**  \n",
    "- **Auxiliary load-balancing loss** to encourage uniform expert utilization.\n",
    "\n",
    "**Key improvements**  \n",
    "- **Top-2 gating** for richer expert selection.  \n",
    "- **Independent attention-layer adapters** for per-layer specialization.  \n",
    "- **40 % GPU memory reduction** and **30 % lower latency**.  \n",
    "- **9 % accuracy gain** on multi-task benchmarks.\n",
    "\n",
    "#### MoE-Mamba: SSM-Infused Sparse MoE (Pióro et al., 2024)\n",
    "\n",
    "**At a glance**  \n",
    "A hybrid SSM/MoE model that interleaves Mamba State-Space Model blocks with true Switch-style sparse experts.\n",
    "\n",
    "**What is an SSM?**  \n",
    "A State-Space Model evolves a hidden state $h_t$ via  \n",
    "$$\n",
    "h_t = A\\,h_{t-1} + B\\,u_t,\\quad\n",
    "y_t = C\\,h_t + D\\,u_t,\n",
    "$$  \n",
    "where $u_t$ is the input and $y_t$ the output.\n",
    "\n",
    "**Innovation in Mamba**  \n",
    "- **Input-dependent parameterization** of $A,B,C,D$ for content-based gating.  \n",
    "- **Parallel-scan (FFT-style) solver** achieving $O(N\\log N)$ inference on long sequences.  \n",
    "- **Diagonal + low-rank decomposition** for compact, efficient long-range modeling.\n",
    "\n",
    "**Switch-inspired elements**  \n",
    "- **Single-expert routing** ($k=1$) with capacity-factor buffering.  \n",
    "- **Auxiliary load-balancing loss** at every MoE layer.  \n",
    "- **Low-precision routing** (float32 jitter) for stable training.\n",
    "\n",
    "**Key improvements**  \n",
    "- **Sequential interleaving** of SSM and MoE blocks.  \n",
    "- **Extensive ablations** on expert count, placement, and parameter ratios.  \n",
    "- **2.35× fewer training steps** to match Mamba perplexity and outperforms Transformer-MoE.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c02a4",
   "metadata": {},
   "source": [
    "### 2.5 Contributions\n",
    "\n",
    "The paper's main contributions are:\n",
    "\n",
    "- Introducing **single-expert (top-1) routing**, drastically simplifying routing and reducing computational overhead.\n",
    "- Presenting clear evidence of performance improvements across diverse NLP benchmarks, validating the model’s practical utility.\n",
    "- Proposing effective model compression through knowledge distillation, enabling deployment of large model capabilities into significantly smaller models.\n",
    "\n",
    "Together, these findings validate the viability of conditional computation at unprecedented scale.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
