{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28700274-faf1-4b4a-ba3c-159a77d81563",
   "metadata": {},
   "source": [
    "# Mixture of experts Language Model presentation\n",
    "\n",
    "## Noam Delbari & Stav Cohen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c93a4-e500-4480-a9fe-16e50791a9f6",
   "metadata": {},
   "source": [
    "## 1 Background:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda35158-1a66-4354-9346-92b5bbd3ef43",
   "metadata": {},
   "source": [
    "### 1.1 Standard Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe019de3-0300-4285-9dcd-6182400f5e56",
   "metadata": {},
   "source": [
    "- Encoder/decoder stacks, multi-head self-attention, position encodings, residual + layer-norm.\n",
    "- Emphasize the Feed-Forward Network (FFN) accounting for ~⅔ of parameters and FLOPs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16ecc2-ad9d-4f1c-9bfe-788abe1f2d04",
   "metadata": {},
   "source": [
    "### 1.2 Mixture-of-Experts (MoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d106ada-a163-49f4-82af-ba3b2dd9bb2a",
   "metadata": {},
   "source": [
    "- Multiple specialist sub-nets compete; a gating network softly/hard-routes input to the best expert(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e70a8c-89fa-4649-83ba-0e205befe346",
   "metadata": {},
   "source": [
    "## 2 Switch transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6283fa5b-7516-441a-95f4-8eb1c6c07fe3",
   "metadata": {},
   "source": [
    "### 2.1 Introduction: Motivation & Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da460eef-f899-48bd-a268-f0a3cd93fb45",
   "metadata": {},
   "source": [
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee1062-c3b6-4348-8786-97806bcf7eb2",
   "metadata": {},
   "source": [
    "### 2.2 Description & Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7863ffea-8996-4830-aa5e-20366d479d78",
   "metadata": {},
   "source": [
    "- Switch Transformer extends the standard Transformer by replacing each dense feed-forward network with a Mixture-of-Experts layer which is being enabled by a lightweight gating network, letting only 1–2 experts process every token. This design keeps inference cost comparable to the dense baseline while unlocking trillion-parameter scale and superior quality on language, translation and multitask benchmarks\n",
    "- Hands-on code snippets of the mode implementation, using equations and explanations: Forward pass, Switch Transformer Block and Switch Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd2cfbd-cef2-4fff-adac-cf97bd4b99dd",
   "metadata": {},
   "source": [
    "### 2.3 Experiments & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9892016-e26e-44c5-8fa9-e5684e2ec41b",
   "metadata": {},
   "source": [
    "- Presenting Scaling & Efficiency results to motivate why use sparse experts\n",
    "- Presenting  Ablation & Stability results to show that gating mechanism is capable for training large scale models\n",
    "- Downstream fine-tuning results to show that the lower perplexity (compared to dense) during pre training has an effect over downstream tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6985c725-08ca-4c7b-9b5e-6c65869db759",
   "metadata": {},
   "source": [
    "### 2.4 Hands-on experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec98893e-7dd9-419b-9438-3f4903c40d75",
   "metadata": {},
   "source": [
    "- Forward pass: Configure single expert per token and plot router decisions (Specificity of token types per expert)\n",
    "- Training: Comparing switch model to dense baseline with equal number of active parameters and show that sparse model reaches lower validation loss faster\n",
    "- Scaling and Efficiency miniature run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57167407-c9a5-429f-87fb-1de5c50a6951",
   "metadata": {},
   "source": [
    "## 3 PEER (parameter efficient expert retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b9f11-e31a-41a5-b8e6-813bf37d7c68",
   "metadata": {},
   "source": [
    "### 3.0 Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a768a-a2b2-4fec-a5ea-af30c20dbfd9",
   "metadata": {},
   "source": [
    "Just to be alligned, we'll explain here the following terms:\n",
    "\n",
    "* **expert** - A specialized model or sub-model intended to handle a specific subset of data or tasks. It can be assumed as a small neural network (Usually some few neurons) which is a sub part of a layer.\n",
    "* **product key** - A mechanism to enable efficient and effective matching between input data and the best-suited experts to process that data in a vast and diverse pool of potential experts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae322a-6a7f-4b44-b5e4-57bc15d704a5",
   "metadata": {},
   "source": [
    "### 3.1 Introduction and motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5675abf-bf46-4439-9efb-4f57ee397fd3",
   "metadata": {},
   "source": [
    "- Based on the following paper: https://arxiv.org/pdf/2407.04153v1.\n",
    "- The main idea here is the innovation of the sparse mixture-of-experts (MoE) model architectures using PEER (parameter efficient expert retrieval) layer.\n",
    "- The PEER layer includes a vast number of tiny expert (over a million) and solves one of the main issues of the traditional models:\n",
    "  - In traditional MoE models, the feedforward layers (FFW) have a linear increase in computational costs and activation memory as the hidden layer width grows.\n",
    "  - However, in PEER, by enabling efficient utilization of a massive number of experts, it can support a further scaling of transformer models while maintaining almost the same computational efficiency.\n",
    "  - It uses the product key technique for sparse retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de9cd4-23cb-4084-9c74-a7fbe21d5623",
   "metadata": {},
   "source": [
    "### 3.2 Architecture and mathematical equalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf6aef6-c0ce-4246-8821-0ab1ca0db463",
   "metadata": {},
   "source": [
    "<img src=\"Peer_Layer.png\" alt=\"Peer_Layer\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19cc5dd-7fe5-4a11-80c4-807d84608e65",
   "metadata": {},
   "source": [
    "The following is an illustration of the PEER layer:\n",
    "* A PEER layer can be inserted in the middle of a transformer backbone or can be used to replace FFW layers\n",
    "* Given the state vector x from the previous layer, a query\n",
    "network $q$ maps it to a query vector $q(x)$, which is then compared with the product keys to compute the\n",
    "router scores and to retrieve the top $k$ experts $e_1$, ..., $e_k$\n",
    "* After the retrieved experts make their predictions\n",
    "ei(x), their outputs are linearly combined using the softmax-normalized router scores as weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132de35-4f8d-40b5-b873-7f9a41bffd7d",
   "metadata": {},
   "source": [
    "Formally, a PEER layer is a function $f:{R^n}\\rightarrow{R^m}$ that consists of three parts:\n",
    "* A pool of $N$ experts $E:=\\{{e_i}\\}_{i=1}^{N}$ where each expert $e_i:{R^n}\\rightarrow{R^m}$ shares the same signature as $f$\n",
    "* A matching set of $N$ produect keys: $K:=\\{{k_i}\\}_{i=1}^{N} \\subset R^d$ ($d$ is the dimension of the vectors)\n",
    "* A query network $q:{R^n}\\rightarrow{R^d}$ that maps the input vector $x\\in{R^n}$ to a query vector $q(x)$\n",
    "\n",
    "The layer output is done by the following three steps:\n",
    "1. Retrieving the top $k$ experts: The experts who their corresponding product keys have the highest inner products with the query $q(x)$: $I=T_k(\\{q(x)^Tk_i\\})_{i=1}^{N}$ while $T_k$ is the top $k$ operator\n",
    "2. Actication: Applying nonlinear activations (such as softmax or sigmoid) to the query-key inner products of these\n",
    "top $k$ experts to obtain the router scores: $g_i(x) = s(q(x)^Tk_i)$\n",
    "3. Output: Computing the output by linearly combining the expert outputs weighted by the router scores: $f(x) = \\sum_{{i}\\in{I}}g_i(x)e_i(x)$.\n",
    "\n",
    "Product key retrieval - Efficiency:\n",
    "* Since $N$ can be a huge number $(N \\geq 10^6)$, we don't want to compute it by the naive way\n",
    "* Instead of using $N$ independent $d$-dimensional vectors as our keys $k_i$, we create them by concatenating vectors from two independent sets of $\\frac{d}{2}$-dimensional sub-keys $C, C' \\subset R^{\\frac{d}{2}}$, $|C|=|C'|=\\sqrt{N}$\n",
    "* So in total $K = {\\{[_{c'}^{c}]|c\\in{C}, c'\\in{C'}\\}}$\n",
    "* Hence we choose $N$ to be a perfect square and $d$ to be an even number\n",
    "* Instead of comparing $q(x)$ to all $N$ keys in $K$ and selecting the top $k$ matches, we split the query vector $q(x)$ into two subqueries $q1$ and $q2$ and apply the top $k$ operations to the inner products between the sub-queries and sub-keys\n",
    "respectively\n",
    "* This results in a set of $k^2$ candidate keys, and it is mathematically guaranteed that the $k$ most similar keys to $q(x)$ from $K$ are in this candidate set so we can simply apply the top-k operator again to these $k^2$ inner products to get the top $k$ matching keys from the original set of product keys $K$\n",
    "* Naive way runtime complexity: $O(Nd)$\n",
    "* Efficient way runtime complexity: $O((\\sqrt{N}+k^2)d)$\n",
    "\n",
    "\n",
    "Parameter Efficient Experts and Multi-Head Retrieval:\n",
    "* In other MoE architectures, the hidden layer of each expert is set to the same size as other FFW layers (A few and large experts)\n",
    "* In PEER, every expert is a one hidden layer with a single neuron $e_i(x) := \\sigma(u_i^Tx)v_i$ while $\\sigma$ is a non-linear activation function\n",
    "* Instead of making the size of individual experts different, we are using a multihead retrieval of $h$ independent query networks instead of one, each computes its own query and retrieves a separate set of $k$ experts: $f(x) := \\sum_{i=1}^{h}f^i(x) = \\sum_{i=1}^{h}\\sum_{j \\in I^i} g^j(x)e^j(x)$\n",
    "\n",
    "Hyperparameters and loss function:\n",
    "* There are three main hyperparameters to a standard MoE layer:\n",
    "  1. $P$ - Total number of parameters\n",
    "  2. $P_{active}$ - Total number of active parameters per token\n",
    "  3. $P_{expert}$ - The size of a single expert\n",
    "* Two more important terms:\n",
    "  1. $D$ - Number of training tokens\n",
    "  2. $G := \\frac{P_{active}}{P_{experts}}$ - Number of active experts\n",
    "* Now the loss function is $L(P, D, G) = c + (\\frac{g}{G^\\lambda} + a)\\frac{1}{P^\\alpha} + \\frac{b}{D^\\beta}$ ($a, b, c, g, \\alpha, \\beta, \\lambda$ are constants)\n",
    "* We would like to scale the total number of parameters, of experts and of the tokens, but NOT the number of active parameters, since the number of the active parameters affects the computational and runtime costs\n",
    "* Since we want to increase the number of experts, we need to decrease the size of each expert if we do not increase the number of active parameters. Hence we need a large number of small experts\n",
    "* In PEER, we set $P_{experts}$ to 1 and $P_{active}$ is the number of retrieval heads multiplied by the number of experts retrieved per head which is $hk$. Hence, in PEER layer the number of active experts is $G = hk$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5012373d-bae6-4434-a232-692cf8c59c60",
   "metadata": {},
   "source": [
    "### 3.3 Results of major experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5015af-e108-4a66-8774-f5ae9a99059f",
   "metadata": {},
   "source": [
    "<img src=\"Peer_Experiment.png\" alt=\"PEER_Experiment\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421858d6-0557-42b3-b9d5-8e35cdf6a3a2",
   "metadata": {},
   "source": [
    "The following is a graph of the train loss per batch of their training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621baeb-678b-4f57-921c-d7ccc874985a",
   "metadata": {},
   "source": [
    "<img src=\"6e18_6e19_FLOPs.png\" alt=\"6e18_6e19_FLOPs\" width=\"700\" height=\"500\">\n",
    "<img src=\"Varying_N_hk.png\" alt=\"Varying_N_hk\" width=\"700\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab50e99-1c66-4491-84d1-2a400f6cd1ac",
   "metadata": {},
   "source": [
    "* The following graphs in the upper side shows the perplexity per number of parameters for 2e19 (more efficient computation) and 2e18 (less efficient computation) FLOP's, comparing different MoE models:\n",
    "  * Dataset is $C4$\n",
    "  * Total number of experts $N$ is $1024^2$\n",
    "  * The number of active experts is $128$ for Dense, PEER and MOE and $256$ memories for PKM\n",
    "  * It can be shown that PEER gives the lower perplexity and with the largest number of parameters\n",
    "* The graph in the left-bottom side shows the perplexity per number of parameters of PEER models, comparing different number of total experts $N$\n",
    "  * The size of the active experts $hk$ is always $128$\n",
    "  * Dataset is $C4$\n",
    "* The graph in the right-bottom side shows the perplexity per number of $h$ independent query networks (Active experts)\n",
    "  * Total number of experts $N$ is always $1024^2$\n",
    "  * Dataset is $C4$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa4e99a-3ae3-46d6-a9c6-26c578c83c20",
   "metadata": {},
   "source": [
    "<img src=\"PEER_Experiment_Table.png\" alt=\"PEER_Experiment_Table\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2321f72-dc8d-4f60-8ccd-ebe239dec7bd",
   "metadata": {},
   "source": [
    "The following table compares the perplexity per model (rows) and per dataset (columns):\n",
    "  * Total number of experts $N$ is $1024^2$\n",
    "  * The number of active experts is $128$ for Dense, PEER and MOE and $256$ memories for PKM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce778b-b88f-481a-b266-2a322a9441c5",
   "metadata": {},
   "source": [
    "<img src=\"BatchNormTable.png\" alt=\"BatchNormTable\" width=\"600\" height=\"600\">\n",
    "<img src=\"BatchNormGraph.png\" alt=\"BatchNormGraph\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf2edd-b70a-433f-b810-d85a38fcf496",
   "metadata": {},
   "source": [
    "The following table and graph shows the perplexity per number of experts $N$ (In the graph the number of experts $N$ is $1M$) and using two scenarios:\n",
    "* With no query batch normalization\n",
    "* With query batch normalization\n",
    "\n",
    "These are the following details of experiments:\n",
    "* Dataset is $C4$\n",
    "* Model is PEER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b5ff6-5f7c-42fa-ace0-fc9eea195c76",
   "metadata": {},
   "source": [
    "### 3.4 Hands-on examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01708c1d-443c-40b1-8c28-112c66e93112",
   "metadata": {},
   "source": [
    "-  Some hands on examples of basic peer layer implementation with the execution output in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a63e7d-904a-496b-a20f-0a7d0171ece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here we're going to make hands-on examples\n"
     ]
    }
   ],
   "source": [
    "print(\"Here we're going to make hands-on examples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
